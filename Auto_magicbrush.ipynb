{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05482068-90ba-44ff-85c0-f399fcb6f1ac",
   "metadata": {},
   "source": [
    "import json\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import base64\n",
    "import requests\n",
    "import pathlib\n",
    "import textwrap\n",
    "import google.generativeai as genai\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "import http.client\n",
    "import typing\n",
    "import urllib.request\n",
    "\n",
    "import IPython.display\n",
    "from PIL import Image as PIL_Image\n",
    "from PIL import ImageOps as PIL_ImageOps\n",
    "\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa32bc2b-ca0c-4aed-9197-02066f97990b",
   "metadata": {},
   "source": [
    "data_path = 'magic_brush/test'"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "256661a0-ebf4-4998-8dba-1e351cb3d1f0",
   "metadata": {},
   "source": [
    "f = open(f'{data_path}/edit_turns.json')\n",
    "instructions = json.load(f)\n",
    "f = open(f'{data_path}/global_descriptions.json')\n",
    "global_descriptions = json.load(f)\n",
    "f = open(f'{data_path}/local_descriptions.json')\n",
    "local_descriptions = json.load(f)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "038a97a5",
   "metadata": {},
   "source": [
    "len(instructions)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "82f6f427-b362-48d3-87d7-eb8dc095d12b",
   "metadata": {},
   "source": [
    "# Create all pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbc6b11",
   "metadata": {},
   "source": [
    "clip_L, clip_L_preprocess = clip.load(\"ViT-L/14\", device='cuda')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2ac7eae5",
   "metadata": {},
   "source": [
    "def compute_similarity(image1, image2, model=clip_L, preprocess=clip_L_preprocess):\n",
    "    image1_preprocess = preprocess(Image.open(image1)).unsqueeze(0).to('cuda')\n",
    "    image1_features = model.encode_image( image1_preprocess)\n",
    "\n",
    "    image2_preprocess = preprocess(Image.open(image2)).unsqueeze(0).to('cuda')\n",
    "    image2_features = model.encode_image( image2_preprocess)\n",
    "    \n",
    "    cos = torch.nn.CosineSimilarity(dim=0)\n",
    "    similarity = cos(image1_features[0],image2_features[0]).item()\n",
    "    #print(\"Image similarity\", similarity)\n",
    "    return similarity"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8138cde8",
   "metadata": {},
   "source": [
    "count = 0\n",
    "data = []\n",
    "for edit_idx, sample in enumerate(instructions):\n",
    "    data.append({\"path_input\": f'{data_path}/images/{sample[\"input\"].split(\"-\")[0]}/{sample[\"input\"]}',\n",
    "    \"input_global\": f'{global_descriptions[sample[\"input\"].split(\"-\")[0]][sample[\"input\"]]}',\n",
    "    \"path_output\":f'{data_path}/images/{sample[\"input\"].split(\"-\")[0]}/{sample[\"output\"]}',\n",
    "    \"output_local\": f'{local_descriptions[sample[\"input\"].split(\"-\")[0]][sample[\"output\"]]}',\n",
    "    \"output_global\": f'{global_descriptions[sample[\"input\"].split(\"-\")[0]][sample[\"output\"]]}',\n",
    "    \"instruction\": sample['instruction'],\n",
    "    \"CLIP_similarity\": compute_similarity(f'{data_path}/images/{sample[\"input\"].split(\"-\")[0]}/{sample[\"input\"]}', f'{data_path}/images/{sample[\"input\"].split(\"-\")[0]}/{sample[\"output\"]}')})\n",
    "    count += 1"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "14471a84",
   "metadata": {},
   "source": [
    "count"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c3a2f582",
   "metadata": {},
   "source": [
    "sorted_data = sorted(data, key=lambda d: d['CLIP_similarity'], reverse=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a46aa2f5",
   "metadata": {},
   "source": [
    "sorted_data[-500:]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8a85f790",
   "metadata": {},
   "source": [
    "with open('magic_brush_test.json', 'w') as fout:\n",
    "    json.dump(sorted_data, fout)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "62546cf3-1d7d-467f-85c1-42ed85ff437f",
   "metadata": {},
   "source": [
    "# Setup models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d997dbbf-5700-44d6-8067-8dfcd15b61a9",
   "metadata": {},
   "source": [
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "headers = {\n",
    "  \"Content-Type\": \"application/json\",\n",
    "  \"Authorization\": f\"Bearer {api_key}\"\n",
    "}\n",
    "gpt = OpenAI()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2e3efeca-5586-4f52-a565-9d1b8b4b43ff",
   "metadata": {},
   "source": [
    "genai.configure(api_key='AIzaSyA_kbfVsa65btu37xRBPb9UyYytEHLKhd8')\n",
    "gemini = genai.GenerativeModel(model_name=\"gemini-1.5-pro-latest\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b4abb7c3-2dce-4f68-bc8e-c9bdb84bd2b3",
   "metadata": {},
   "source": [
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "14e991d4-92d2-44fb-b8e4-fb8d86c564a0",
   "metadata": {},
   "source": [
    "def gpt_inference_two(image_1, image_2, prompt):\n",
    "    en_1  = encode_image(image_1)\n",
    "    en_2 = encode_image(image_2)\n",
    "    payload = {\n",
    "          \"model\": \"gpt-4-turbo\",\n",
    "          \"messages\": [\n",
    "            {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": [\n",
    "                {\n",
    "                  \"type\": \"text\",\n",
    "                  \"text\": prompt\n",
    "                },\n",
    "                {\n",
    "                  \"type\": \"image_url\",\n",
    "                  \"image_url\": {\n",
    "                    \"url\": f\"data:image/jpeg;base64,{en_1}\"\n",
    "                  }\n",
    "                },\n",
    "                {\n",
    "                  \"type\": \"image_url\",\n",
    "                  \"image_url\": {\n",
    "                    \"url\": f\"data:image/jpeg;base64,{en_2}\"\n",
    "                  }\n",
    "                }\n",
    "              ]\n",
    "            }\n",
    "          ],\n",
    "          \"max_tokens\": 300\n",
    "        }\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "    answer = response.json()['choices'][0]['message']['content']\n",
    "    return answer"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "756067ac",
   "metadata": {},
   "source": [
    "def gpt_inference_one(image, prompt):\n",
    "    en  = encode_image(image)\n",
    "    payload = {\n",
    "          \"model\": \"gpt-4-turbo\",\n",
    "          \"messages\": [\n",
    "            {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": [\n",
    "                {\n",
    "                  \"type\": \"text\",\n",
    "                  \"text\": prompt\n",
    "                },\n",
    "                {\n",
    "                  \"type\": \"image_url\",\n",
    "                  \"image_url\": {\n",
    "                    \"url\": f\"data:image/jpeg;base64,{en}\"\n",
    "                  }\n",
    "                }\n",
    "              ]\n",
    "            }\n",
    "          ],\n",
    "          \"max_tokens\": 300\n",
    "        }\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "    answer = response.json()['choices'][0]['message']['content']\n",
    "    return answer"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "43cbd9c3",
   "metadata": {},
   "source": [
    "prompt = '''\n",
    "I want to extract as many components as possible from the provided images. Component examples are shown below. However, components are not limited to the following components. Please only provide the component name without any explanation and separate the component names with commons. If a human or an animal is shown in the images and hair, eye, hand, mouth, ear, and leg, etc. are visible, ensure to include them, Similarly, try to find all the components as detailed as possible. \n",
    "1. leg, 2. eye, 3. ear, 4. food, 5. pillow, 6. flower, 7. plate, 8. window, 9. door, 10. chair, 11. dining table, 12. sofa, 13. banana, 14. bowl, 15. sugar, 16. blender, 17. berry, 18. lizard, 19. watermelon, 20. motorcycle, 21. apple, 22. curtain, 23, cookies, 24, cake, 25. hair, 26, hat, 27, dresses, 28. bacon, 29. butter, 30, jam, 31, bread 32, surfboard, 33, t-shirt, 34, pants, 35, hands, 36. fridge, 37, plants, 38. cabinet, 39, sink, 40, car, 41, girl, 42, boy\n",
    "'''"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "78357d4d",
   "metadata": {},
   "source": [
    "option_data = []\n",
    "for idx, pair in enumerate(sorted_data):\n",
    "    if pair['CLIP_similarity'] >= 0.94:\n",
    "        answer = gpt_inference_one(pair['path_output'], prompt)\n",
    "        pair['GPT_option'] = answer\n",
    "    else:\n",
    "        pair['GPT_option'] = None\n",
    "    option_data.append(pair)\n",
    "    print(idx)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9b41033d",
   "metadata": {},
   "source": [
    "answer"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "77167fe1",
   "metadata": {},
   "source": [
    "option_data"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "35c19297",
   "metadata": {},
   "source": [
    "instruction_data = []\n",
    "for pair in option_data:\n",
    "    pair['instruction'] = tmp[pair['path_input']]\n",
    "    instruction_data.append(pair)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "acb74983",
   "metadata": {},
   "source": [
    "with open('magic_brush_test.json', 'w') as fout:\n",
    "    json.dump(option_data, fout)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc42790d",
   "metadata": {},
   "source": [
    "val = json.load(open('/home/marco/PhD/VL_fine-grained/magic_brush_val_94clip.json'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43d66ce4",
   "metadata": {},
   "source": [
    "counter = 0\n",
    "for i in val:\n",
    "    if i['GPT_option'] is not None:\n",
    "        counter += 1"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6db2a20c",
   "metadata": {},
   "source": [
    "val[314]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f870bd1",
   "metadata": {},
   "source": [
    "counter"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f48bf25-2bd2-43c0-aa68-6a6f30bf3cd5",
   "metadata": {},
   "source": [
    "f, axarr = plt.subplots(2, 1, figsize=(15, 15))\n",
    "f.tight_layout(pad=5.0)\n",
    "axarr[0].imshow(mpimg.imread(val[314]['path_input']))\n",
    "axarr[1].imshow(mpimg.imread(val[314]['path_output']))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4ae495-c015-4ce1-a693-f66301e46793",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vl_compare",
   "language": "python",
   "name": "vl_compare"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
